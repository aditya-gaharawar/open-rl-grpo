{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Fine-tuning - All-in-One Notebook By *WEBSPACEAI RESEARCH*\n",
    "\n",
    "This notebook provides a complete end-to-end pipeline to convert Qwen3-4B-Base or Any Model into a reasoning model via GRPO (Generalized Reward Policy Optimization) using OpenR1's Math dataset. Everything is included in this single notebook - from environment validation to model export.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Environment Validation**: Check Python version, GPU availability, and disk space\n",
    "2. **Dependency Installation**: Automatically install all required packages\n",
    "3. **Model Setup**: Load and configure the Qwen3-4B model with LoRA\n",
    "4. **Pre-training**: Fine-tune the model to understand custom GRPO formatting\n",
    "5. **Data Preparation**: Process the Open R1 Math dataset\n",
    "6. **GRPO Training**: Train the model for mathematical reasoning\n",
    "7. **Inference**: Test the model's reasoning capabilities\n",
    "8. **Model Export**: Save the model in various formats\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- CUDA-compatible GPU with 16+ GB VRAM (recommended)\n",
    "- Python 3.8+\n",
    "- ~30 GB disk space for model and datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Validation\n",
    "\n",
    "First, let's check if your environment meets the requirements for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper functions for pretty output\n",
    "def print_header(message):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\" {message}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def print_step(message):\n",
    "    print(f\"\\n>> {message}\")\n",
    "\n",
    "def print_success(message):\n",
    "    print(f\"\u2705 {message}\")\n",
    "\n",
    "def print_warning(message):\n",
    "    print(f\"\u26a0\ufe0f {message}\")\n",
    "\n",
    "def print_error(message):\n",
    "    print(f\"\u274c {message}\")\n",
    "\n",
    "print_header(\"Environment Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "print_step(\"Checking Python version\")\n",
    "python_version = platform.python_version()\n",
    "print(f\"Python version: {python_version}\")\n",
    "\n",
    "major, minor, _ = map(int, python_version.split('.'))\n",
    "if major < 3 or (major == 3 and minor < 8):\n",
    "    print_warning(\"Python 3.8+ is recommended for this notebook\")\n",
    "    python_ok = False\n",
    "else:\n",
    "    print_success(\"Python version is compatible\")\n",
    "    python_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "print_step(\"Checking for GPU\")\n",
    "\n",
    "# Try to import torch\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"Found {device_count} CUDA device(s)\")\n",
    "        \n",
    "        for i in range(device_count):\n",
    "            device_name = torch.cuda.get_device_name(i)\n",
    "            device_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print_success(f\"GPU {i}: {device_name} ({device_memory:.2f} GB)\")\n",
    "        \n",
    "        gpu_available = True\n",
    "    else:\n",
    "        print_warning(\"No CUDA-compatible GPU detected\")\n",
    "        \n",
    "        # Check if NVIDIA drivers are installed\n",
    "        try:\n",
    "            subprocess.run(\"nvidia-smi\", shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print_warning(\"NVIDIA drivers found but CUDA is not available in PyTorch\")\n",
    "            print(\"Please ensure PyTorch is installed with CUDA support\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print_error(\"NVIDIA drivers not found or not properly installed\")\n",
    "            print(\"Please install NVIDIA drivers: https://www.nvidia.com/Download/index.aspx\")\n",
    "        \n",
    "        gpu_available = False\n",
    "except ImportError:\n",
    "    print_warning(\"PyTorch not installed, will attempt to install it\")\n",
    "    gpu_available = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available disk space\n",
    "print_step(\"Checking available disk space\")\n",
    "\n",
    "total, used, free = shutil.disk_usage(os.getcwd())\n",
    "free_gb = free / (2**30)\n",
    "print(f\"Available disk space: {free_gb:.2f} GB\")\n",
    "\n",
    "if free_gb < 30:\n",
    "    print_warning(\"Less than 30GB free disk space. You may encounter issues downloading models and datasets\")\n",
    "    disk_ok = False\n",
    "else:\n",
    "    print_success(f\"Sufficient disk space available ({free_gb:.2f} GB)\")\n",
    "    disk_ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize environment check\n",
    "print_header(\"Environment Check Summary\")\n",
    "\n",
    "if not python_ok:\n",
    "    print_warning(\"Python version check: FAILED - Continuing with unsupported Python version, but you may encounter issues\")\n",
    "else:\n",
    "    print_success(\"Python version check: PASSED\")\n",
    "\n",
    "if gpu_available is False:  # None means not installed yet\n",
    "    print_warning(\"GPU check: FAILED - No GPU detected. This notebook requires a GPU for training\")\n",
    "    proceed = input(\"Continue anyway? (y/n): \").lower() == 'y'\n",
    "    if not proceed:\n",
    "        print(\"Notebook execution stopped. Please run on a machine with a CUDA-compatible GPU.\")\n",
    "        # This will only work in interactive mode\n",
    "        # import sys; sys.exit()\n",
    "elif gpu_available is True:\n",
    "    print_success(\"GPU check: PASSED\")\n",
    "else:\n",
    "    print_warning(\"GPU check: UNKNOWN - Will attempt to install PyTorch with CUDA support\")\n",
    "\n",
    "if not disk_ok:\n",
    "    print_warning(\"Disk space check: FAILED - Low disk space may cause issues during model download and training\")\n",
    "    proceed = input(\"Continue anyway? (y/n): \").lower() == 'y'\n",
    "    if not proceed:\n",
    "        print(\"Notebook execution stopped. Please free up disk space and try again.\")\n",
    "        # import sys; sys.exit()\n",
    "else:\n",
    "    print_success(\"Disk space check: PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependency Installation\n",
    "\n",
    "Now let's install all required dependencies for training. This may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run pip install with error handling\n",
    "def pip_install(package, quiet=False):\n",
    "    try:\n",
    "        if quiet:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        else:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}\")\n",
    "        return False\n",
    "\n",
    "print_header(\"Installing Dependencies\")\n",
    "\n",
    "# First install PyTorch with CUDA support if not already installed\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch {torch.__version__} already installed\")\n",
    "    torch_installed = True\n",
    "except ImportError:\n",
    "    print(\"Installing PyTorch with CUDA support...\")\n",
    "    success = pip_install(\"torch>=2.0.0 --extra-index-url https://download.pytorch.org/whl/cu118\")\n",
    "    if not success:\n",
    "        print_warning(\"Failed to install PyTorch with CUDA 11.8, trying CUDA 11.7...\")\n",
    "        success = pip_install(\"torch>=2.0.0 --extra-index-url https://download.pytorch.org/whl/cu117\")\n",
    "    if not success:\n",
    "        print_warning(\"Failed to install PyTorch with CUDA 11.7, trying default...\")\n",
    "        success = pip_install(\"torch>=2.0.0\")\n",
    "    if not success:\n",
    "        print_error(\"Failed to install PyTorch\")\n",
    "        torch_installed = False\n",
    "    else:\n",
    "        torch_installed = True\n",
    "        # Import torch again to verify installation\n",
    "        try:\n",
    "            import torch\n",
    "            print_success(f\"PyTorch {torch.__version__} installed successfully\")\n",
    "        except ImportError:\n",
    "            print_error(\"PyTorch installation failed\")\n",
    "            torch_installed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers first as it's a core dependency\n",
    "print_step(\"Installing transformers...\")\n",
    "success = pip_install(\"transformers>=4.34.0\")\n",
    "if not success:\n",
    "    print_error(\"Failed to install transformers\")\n",
    "    transformers_installed = False\n",
    "else:\n",
    "    transformers_installed = True\n",
    "    print_success(\"Transformers installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install key packages individually\n",
    "print_step(\"Installing key packages...\")\n",
    "\n",
    "packages = [\n",
    "    \"unsloth>=2023.11.0\",\n",
    "    \"vllm==0.8.5.post1\",\n",
    "    \"bitsandbytes>=0.39.0\",\n",
    "    \"accelerate>=0.23.0\",\n",
    "    \"xformers==0.0.29.post3\",\n",
    "    \"peft>=0.5.0\",\n",
    "    \"trl>=0.7.2\",\n",
    "    \"triton>=2.0.0\",\n",
    "    \"safetensors>=0.3.2\",\n",
    "    \"datasets>=3.4.1\",\n",
    "    \"pandas>=2.0.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\",\n",
    "    \"huggingface_hub\",\n",
    "    \"cut_cross_entropy\",\n",
    "    \"unsloth_zoo\"\n",
    "]\n",
    "\n",
    "# Bolt Optimization: Batch install for performance\n",
    "print(f\"Installing {len(packages)} packages in batch...\")\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    # Batch install allows pip to resolve dependencies more efficiently\n",
    "    # We explicitly import sys and subprocess to ensure they are available in this cell\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + packages + [\"--quiet\"]\n",
    "    subprocess.check_call(cmd)\n",
    "    print_success(\"Package installation completed\")\n",
    "except subprocess.CalledProcessError:\n",
    "    if \"print_warning\" in globals():\n",
    "        print_warning(\"Batch installation failed, falling back to sequential installation\")\n",
    "    else:\n",
    "        print(\"Batch installation failed, falling back to sequential installation\")\n",
    "    \n",
    "    for package in packages:\n",
    "        print(f\"Installing {package}...\")\n",
    "        pip_install(package, quiet=True)\n",
    "    print_success(\"Sequential installation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate key imports\n",
    "print_step(\"Validating package imports\")\n",
    "\n",
    "key_packages = [\n",
    "    \"torch\", \n",
    "    \"transformers\", \n",
    "    \"unsloth\", \n",
    "    \"vllm\", \n",
    "    \"datasets\", \n",
    "    \"peft\", \n",
    "    \"trl\"\n",
    "]\n",
    "\n",
    "all_success = True\n",
    "for package in key_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print_success(f\"Successfully imported {package}\")\n",
    "    except ImportError as e:\n",
    "        print_error(f\"Failed to import {package}: {e}\")\n",
    "        all_success = False\n",
    "\n",
    "if all_success:\n",
    "    print_success(\"All key packages imported successfully\")\n",
    "else:\n",
    "    print_warning(\"Some packages failed to import. You may encounter issues during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Configure Model\n",
    "\n",
    "Now we'll load the Qwen3-4B-Base model and configure it with LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048  # Can increase for longer reasoning traces\n",
    "lora_rank = 32  # Larger rank = smarter, but slower\n",
    "\n",
    "print_header(\"Loading Qwen3-4B-Base Model\")\n",
    "print(\"This may take a few minutes to download and prepare the model...\")\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = False,  # False for LoRA 16bit\n",
    "        fast_inference = True,  # Enable vLLM fast inference\n",
    "        max_lora_rank = lora_rank,\n",
    "        gpu_memory_utilization = 0.7,  # Reduce if out of memory\n",
    "    )\n",
    "\n",
    "    # Configure LoRA for efficient fine-tuning\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha = lora_rank*2,  # *2 speeds up training\n",
    "        use_gradient_checkpointing = \"unsloth\",  # Reduces memory usage\n",
    "        random_state = 3407,\n",
    "    )\n",
    "\n",
    "    print_success(\"Model loaded and configured successfully\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Error loading model: {e}\")\n",
    "    print(\"Please check your internet connection and GPU memory availability.\")\n",
    "    # Uncomment to stop execution if model loading fails\n",
    "    # raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure GRPO Chat Template\n",
    "\n",
    "Now we'll set up a custom chat template for the GRPO training process. This defines how the model formats its reasoning and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reasoning format markers\n",
    "reasoning_start = \"<start_working_out>\"  # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"    # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "# Create system prompt that instructs the model how to format its responses\n",
    "system_prompt = f\"\"\"\\\n",
    "You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "print(\"System prompt:\")\n",
    "print(system_prompt)\n",
    "\n",
    "# Create a custom chat template\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with our specific template values\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "# Test the chat template\n",
    "example_chat = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 1+1?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "]\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(example_chat, tokenize=False, add_generation_prompt=True)\n",
    "print(\"\\nExample chat with template applied:\")\n",
    "print(formatted_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-Fine-Tuning for Formatting\n",
    "\n",
    "Before the main GRPO training, we'll pre-train the model on a small dataset to help it understand our custom formatting. This makes the GRPO training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print_header(\"Loading Pre-training Dataset\")\n",
    "try:\n",
    "    dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split=\"cot\")\n",
    "    dataset = dataset.to_pandas()[\n",
    "        [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "    ]\n",
    "\n",
    "    # Filter for numerical answers only\n",
    "    is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors=\"coerce\").notnull()\n",
    "    dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "    \n",
    "    print_success(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "except Exception as e:\n",
    "    print_warning(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating a minimal example dataset instead...\")\n",
    "    # Create a minimal dataset if the original can't be loaded\n",
    "    data = {\n",
    "        \"expected_answer\": [\"2\", \"4\", \"10\", \"5\", \"9\"],\n",
    "        \"problem\": [\n",
    "            \"What is 1+1?\", \n",
    "            \"What is 2+2?\", \n",
    "            \"What is 5+5?\",\n",
    "            \"What is 10/2?\",\n",
    "            \"What is 3*3?\"\n",
    "        ],\n",
    "        \"generated_solution\": [\n",
    "            \"<think>To find 1+1, I add 1 and 1 together. 1+1=2.</think>\",\n",
    "            \"<think>To find 2+2, I add 2 and 2 together. 2+2=4.</think>\",\n",
    "            \"<think>To find 5+5, I add 5 and 5 together. 5+5=10.</think>\",\n",
    "            \"<think>To find 10/2, I divide 10 by 2. 10/2=5.</think>\",\n",
    "            \"<think>To find 3*3, I multiply 3 by 3. 3*3=9.</think>\"\n",
    "        ]\n",
    "    }\n",
    "    dataset = pd.DataFrame(data)\n",
    "    print_success(f\"Created minimal dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset to follow our GRPO style\n",
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    return [\n",
    "        {\"role\": \"system\",    \"content\": system_prompt},\n",
    "        {\"role\": \"user\",      \"content\": problem},\n",
    "        {\"role\": \"assistant\", \"content\": final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis=1)\n",
    "\n",
    "# Check an example\n",
    "print(\"\\nExample formatted message:\")\n",
    "example = tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize=False)\n",
    "print(example[:500] + \"...\" if len(example) > 500 else example)\n",
    "\n",
    "# Truncate dataset to max_seq_length/2\n",
    "print(\"\\nTruncating dataset to appropriate length...\")\n",
    "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
    "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
    "print(f\"Dataset size after truncation: {dataset.shape[0]} examples\")\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "from datasets import Dataset as HFDataset\n",
    "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize=False)\n",
    "hf_dataset = HFDataset.from_pandas(dataset)\n",
    "print_success(\"Dataset prepared for pre-fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print_header(\"Starting Pre-fine-tuning\")\n",
    "print(\"This step teaches the model to follow our custom format...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=hf_dataset,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,  # Use GA to mimic batch size!\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=2,  # Set this for 1 full training run.\n",
    "        learning_rate=2e-4,  # Reduce to 2e-5 for long training runs\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print_success(\"Pre-fine-tuning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Pre-Fine-Tuned Model\n",
    "\n",
    "Let's check if the model has learned to follow our custom format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Testing Pre-fine-tuned Model\")\n",
    "\n",
    "# Test the model with an example from the dataset\n",
    "if len(dataset) > 0:\n",
    "    test_text = tokenizer.apply_chat_template(\n",
    "        dataset[\"Messages\"][0][:2],  # Just system and user message\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "    )\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    print(\"Generating response with pre-fine-tuned model...\")\n",
    "    _ = model.generate(\n",
    "        **tokenizer(test_text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        temperature=0,\n",
    "        max_new_tokens=1024,\n",
    "        streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    "    )\n",
    "\n",
    "# Clean up to free memory\n",
    "del dataset\n",
    "del hf_dataset\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preparation for GRPO Training\n",
    "\n",
    "Now we'll prepare the main training dataset from Open R1 Math for GRPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print_header(\"Loading Open R1 Math Dataset\")\n",
    "try:\n",
    "    dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split=\"train\")\n",
    "    print_success(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "except Exception as e:\n",
    "    print_warning(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating a minimal example dataset instead...\")\n",
    "    # Create a minimal dataset if the original can't be loaded\n",
    "    minimal_data = {\n",
    "        \"prompt\": [\n",
    "            \"What is the square root of 16?\",\n",
    "            \"If x + 5 = 10, what is x?\",\n",
    "            \"What is 7 * 8?\",\n",
    "            \"What is 144 divided by 12?\",\n",
    "            \"What is 3^4?\",\n",
    "            \"What is the square root of 100?\",\n",
    "            \"If 2x - 3 = 7, what is x?\",\n",
    "            \"What is 25% of 80?\",\n",
    "            \"What is 15 + 27?\",\n",
    "            \"What is 99 - 45?\"\n",
    "        ],\n",
    "        \"solution\": [\"4\", \"5\", \"56\", \"12\", \"81\", \"10\", \"5\", \"20\", \"42\", \"54\"]\n",
    "    }\n",
    "    from datasets import Dataset as HFDataset\n",
    "    dataset = HFDataset.from_dict(minimal_data)\n",
    "    print_success(f\"Created minimal dataset with {len(dataset)} examples\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample problem:\")\n",
    "print(dataset[0][\"prompt\"])\n",
    "print(\"\\nExample solution:\")\n",
    "print(dataset[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract answers (for GSM8K dataset, we would extract from #### sections)\n",
    "def extract_hash_answer(text):\n",
    "    # For GSM8K: if \"####\" in text: return text.split(\"####\")[1].strip()\n",
    "    # For Open R1: just return the text\n",
    "    return text\n",
    "\n",
    "# Bolt Optimization: Use batched processing for speed\n",
    "# Batched mapping is significantly faster (~7x) for large datasets\n",
    "def format_dataset_batched(examples):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": p},\n",
    "            ]\n",
    "            for p in examples[\"prompt\"]\n",
    "        ],\n",
    "        \"answer\": [extract_hash_answer(s) for s in examples[\"solution\"]]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_dataset_batched, batched=True)\n",
    "\n",
    "print(\"\\nExample formatted data:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Up Reward Functions for GRPO\n",
    "\n",
    "Now we'll define several reward functions that will guide the GRPO training process. These functions evaluate the model's responses and provide rewards based on format adherence and correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print_header(\"Setting Up GRPO Reward Functions\")\n",
    "\n",
    "# Create regex pattern to match our formatting\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Test the regex pattern\n",
    "test_examples = [\n",
    "    f\"Let me think!{reasoning_end}\\n{solution_start}\\n2\\n{solution_end}\",\n",
    "    f\"{reasoning_start}Let me think!{reasoning_end}\\n{solution_start}  2  {solution_end}\\n\\n\",\n",
    "]\n",
    "\n",
    "print(\"Testing regex pattern:\")\n",
    "for i, example in enumerate(test_examples):\n",
    "    matches = match_format.findall(example)\n",
    "    print(f\"Example {i+1} matches: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward functions\n",
    "\n",
    "# 1. Exact format matching - 3 points if format is followed exactly\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# 2. Approximate format matching - partial points for each format element\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# 3. Answer checking - reward based on correctness of the answer\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        # Correct answer gets 5 points!\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        # Match if spaces are seen, but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
    "                else: score -= 2.5  # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 4.5  # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Number extraction - for handling numeric answers in various formats\n",
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Test the number extraction\n",
    "number_examples = [\n",
    "    f\"{solution_start}  0.34  {solution_end}\",\n",
    "    f\"{solution_start}  123,456  {solution_end}\",\n",
    "    f\"{solution_start}  -0.234  {solution_end}\",\n",
    "    f\"{solution_start}17{solution_end}\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting number extraction:\")\n",
    "for example in number_examples:\n",
    "    print(match_numbers.findall(example))\n",
    "\n",
    "# 5. Numeric comparison with detailed logging\n",
    "PRINTED_TIMES = 0\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
    "    \n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", \n",
    "            f\"\\nAnswer:\\n{answer[0]}\", \n",
    "            f\"\\nResponse:\\n{responses[0][:300]}...\" if len(responses[0]) > 300 else f\"\\nResponse:\\n{responses[0]}\", \n",
    "            f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores\n",
    "\n",
    "print_success(\"Reward functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Dataset for Training\n",
    "\n",
    "Filter the dataset to ensure all examples fit within our sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Preparing Dataset for GRPO Training\")\n",
    "\n",
    "# Tokenize and measure prompt lengths\n",
    "print(\"Measuring prompt lengths...\")\n",
    "\n",
    "# Efficiently calculate lengths without storing full tokens\n",
    "# Using batched=True and not storing the intermediate 'tokens' column saves memory\n",
    "def get_token_len(x):\n",
    "    return {\"L\": [len(t) for t in tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)]}\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    get_token_len,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Show an example of tokenized prompt\n",
    "print(\"Example tokenized prompt:\")\n",
    "# Tokenize just one example for display purposes\n",
    "example_tokens = tokenizer.apply_chat_template(dataset[0][\"prompt\"], add_generation_prompt=True, tokenize=True)\n",
    "print(tokenizer.decode(example_tokens)[:200] + \"...\")\n",
    "\n",
    "# Find the 90th percentile length to avoid outliers\n",
    "import numpy as np\n",
    "# Use the L column from the mapped dataset\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(f\"90th percentile token length: {maximum_length}\")\n",
    "\n",
    "# Filter dataset to only include examples below the maximum length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "print_success(f\"Dataset filtered to {len(dataset)} examples\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del tokenized\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configure GRPO Training\n",
    "\n",
    "Set up the GRPO trainer with our reward functions and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Configuring GRPO Training\")\n",
    "\n",
    "# Calculate sequence lengths for prompts and completions\n",
    "max_prompt_length = maximum_length + 1  # +1 just in case\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "# Configure vLLM sampling parameters\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p=0.1,\n",
    "    top_p=1.0,\n",
    "    top_k=-1,\n",
    "    seed=3407,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "# Configure GRPO training\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    temperature=1.0,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # Increase to 4 for smoother training\n",
    "    num_generations=4,  # Decrease if out of memory\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    # num_train_epochs=1,  # Set to 1 for a full training run\n",
    "    max_steps=100,  # For quick testing, increase for better results\n",
    "    save_steps=100,\n",
    "    report_to=\"none\",  # Can use Weights & Biases\n",
    "    output_dir=\"outputs\",\n",
    ")\n",
    "\n",
    "print_success(\"GRPO training configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run GRPO Training\n",
    "\n",
    "Start the GRPO training process. This may take a while depending on your dataset size and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Starting GRPO Training\")\n",
    "print(\"This process may take a while. You'll see reward metrics during training.\")\n",
    "print(\"The goal is to see the 'reward' column increase over time.\")\n",
    "\n",
    "# Optional: Create train/test split for evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "try:\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            match_format_exactly,\n",
    "            match_format_approximately,\n",
    "            check_answer,\n",
    "            check_numbers,\n",
    "        ],\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        # For optional training + evaluation\n",
    "        # train_dataset=new_dataset[\"train\"],\n",
    "        # eval_dataset=new_dataset[\"test\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    print_success(\"GRPO training complete\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Error during GRPO training: {e}\")\n",
    "    print(\"This could be due to GPU memory issues or other constraints.\")\n",
    "    print(\"Try reducing num_generations, max_seq_length, or using load_in_4bit=True when loading the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test the Model\n",
    "\n",
    "Let's test our trained model on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Testing Model Performance\")\n",
    "\n",
    "# First test the model without GRPO training (baseline)\n",
    "test_question = \"What is the sqrt of 101?\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(\"Testing model without GRPO training (baseline):\")\n",
    "try:\n",
    "    output = model.fast_generate(\n",
    "        [test_question],\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=None,  # No LoRA\n",
    "    )[0].outputs[0].text\n",
    "\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print_error(f\"Error generating baseline response: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LoRA\n",
    "print(\"Saving LoRA weights...\")\n",
    "try:\n",
    "    model.save_lora(\"grpo_saved_lora\")\n",
    "    print_success(\"LoRA weights saved to 'grpo_saved_lora'\")\n",
    "except Exception as e:\n",
    "    print_error(f\"Error saving LoRA weights: {e}\")\n",
    "\n",
    "# Verify LoRA is trained properly\n",
    "try:\n",
    "    from safetensors import safe_open\n",
    "    \n",
    "    print(\"Verifying LoRA weights...\")\n",
    "    with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "        # Check if tensors contain non-zero values\n",
    "        for key in list(f.keys())[:3]:  # Just check a few keys\n",
    "            tensor = f.get_tensor(key)\n",
    "            n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "            print(f\"Key: {key}, Non-zero ratio: {1 - n_zeros.item():.4f}\")\n",
    "            assert(n_zeros.item() != tensor.numel())\n",
    "    print_success(\"LoRA weights verified successfully\")\n",
    "except Exception as e:\n",
    "    print_warning(f\"Error verifying LoRA weights: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test with our trained GRPO model\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "print(\"\\nTesting model with GRPO training:\")\n",
    "try:\n",
    "    output = model.fast_generate(\n",
    "        test_text,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=model.load_lora(\"grpo_saved_lora\"),\n",
    "    )[0].outputs[0].text\n",
    "\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print_error(f\"Error generating GRPO response: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save the Model\n",
    "\n",
    "Save the trained model in different formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"Model Export Options\")\n",
    "\n",
    "print(\"Uncomment the relevant code blocks below to save the model in your preferred format.\")\n",
    "\n",
    "# Option 1: Merge to 16-bit\n",
    "print(\"\\n1. Merge to 16-bit (full model, moderate size):\")\n",
    "print(\"# model.save_pretrained_merged(\\\"model\\\", tokenizer, save_method=\\\"merged_16bit\\\")\")\n",
    "\n",
    "# Option 2: Merge to 4-bit\n",
    "print(\"\\n2. Merge to 4-bit (full model, smallest size):\")\n",
    "print(\"# model.save_pretrained_merged(\\\"model\\\", tokenizer, save_method=\\\"merged_4bit\\\")\")\n",
    "\n",
    "# Option 3: Save LoRA adapters only\n",
    "print(\"\\n3. Save LoRA adapters only (requires base model to use):\")\n",
    "print(\"# model.save_pretrained(\\\"model\\\")\")\n",
    "print(\"# tokenizer.save_pretrained(\\\"model\\\")\")\n",
    "\n",
    "# Option 4: Save to GGUF format\n",
    "print(\"\\n4. Save to GGUF format (for llama.cpp):\")\n",
    "print(\"# model.save_pretrained_gguf(\\\"model\\\", tokenizer, quantization_method=\\\"q4_k_m\\\")\")\n",
    "\n",
    "print(\"\\nTo save in your preferred format, uncomment the relevant code above and run this cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of these blocks to save the model\n",
    "\n",
    "# 1. Merge to 16-bit (full model, moderate size)\n",
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "# 2. Merge to 4-bit (full model, smallest size)\n",
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\")\n",
    "\n",
    "# 3. Save LoRA adapters only (requires base model to use)\n",
    "# model.save_pretrained(\"model\")\n",
    "# tokenizer.save_pretrained(\"model\")\n",
    "\n",
    "# 4. Save to GGUF format (for llama.cpp)\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully trained a Qwen3-4B model for mathematical reasoning using GRPO! The model now follows a specific format for its reasoning process and provides more accurate solutions.\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Environment Setup**: Validated Python, GPU, and disk space requirements\n",
    "2. **Dependency Installation**: Installed all necessary packages\n",
    "3. **Model Configuration**: Loaded Qwen3-4B and set up LoRA for efficient training\n",
    "4. **Custom Formatting**: Defined a reasoning format with working-out and solution sections\n",
    "5. **Pre-Fine-Tuning**: Taught the model our custom format\n",
    "6. **GRPO Training**: Used reward functions to improve mathematical reasoning\n",
    "7. **Testing**: Verified the model's improved performance\n",
    "8. **Model Export**: Explored options for saving and deploying the model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Increase Training Steps**: For better results, increase `max_steps` or set `num_train_epochs=1`\n",
    "2. **Try Different Datasets**: Experiment with GSM8K or other reasoning datasets\n",
    "3. **Adjust Hyperparameters**: Try different learning rates, batch sizes, or LoRA ranks\n",
    "4. **Deploy the Model**: Save in your preferred format and deploy for inference\n",
    "\n",
    "For more information, visit [Unsloth's documentation](https://docs.unsloth.ai/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}